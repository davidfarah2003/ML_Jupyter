{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: PCA - Solutions\n",
    "In this exercise, we will implement and see the workings of a dimensionality reduction technique: Prinical Component Analysis (PCA).\n",
    "\n",
    "We will also compare it to Fisher's Linear Discriminant Analysis (LDA) on MNIST.\n",
    "\n",
    "Note: these solutions are partial as you will have to implement PCA for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good to import few packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Toy Dataset\n",
    "Let see the PCA results on a toy dataset: `iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data = iris['data'].astype(np.float32)\n",
    "labels = iris['target'] \n",
    "cls_names = iris['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first visualize the trends of different features together. One can see that one class is well separated from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "count = 1\n",
    "colors = np.array([[0.85, 0.85, 0], [0, 0.5, 0], [0.25, 0.25, 1]])\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.subplot(4, 4, count)\n",
    "        for ind, name in enumerate(cls_names):\n",
    "            filtered_class = labels == ind\n",
    "            plt.scatter(data[filtered_class,i], data[filtered_class,j], c=colors[ind,None], label=name)\n",
    "        plt.xlabel(f'feature_{i}')\n",
    "        plt.ylabel(f'feature_{j}')\n",
    "        plt.legend()\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 PCA\n",
    "In the Iris dataset, we have 4 features per data point. Let's now try to reduce the dimensionality from $D=4$ to $d=2$ using PCA. \n",
    "As seen in class, for a dataset $\\mathbf{X}\\in \\mathbb{R}^{N\\times D}$ and for a 1D projection ${\\bf w}_{1}$, PCA solves the following optimization problem\n",
    "$$\\begin{align}\n",
    "    \\max_{\\mathbf{w}_{1}} \\mathbf{w}_{1}^T\\mathbf{C}\\mathbf{w}_{1}\\\\\n",
    "    s.t. ~~~~~ \\mathbf{w}_{1}^T\\mathbf{w}_{1} = 1\n",
    "\\end{align}$$\n",
    "   \n",
    " where $\\mathbf{C}$ is the data covariance matrix\n",
    "$$\\begin{align}\n",
    "         \\mathbf{C} &= \\frac{1}{N}\\sum_{i=0}^{N-1}(\\mathbf{x}_i-\\mathbf{\\bar{x}})(\\mathbf{x}_i-\\mathbf{\\bar{x}})^T\\\\\n",
    "         \\mathbf{\\bar{x}} &= \\frac{1}{N}\\sum_{i=0}^{N-1} \\mathbf{x}_i\n",
    "\\end{align}$$\n",
    "     \n",
    " and $\\mathbf{w}_{1}\\in \\mathbb{R}^{D}$ is the projection vector we are looking for, $\\mathbf{x}\\in \\mathbb{R}^{D}$ is one data sample, and $\\mathbf{\\bar{x}} = \\tfrac{1}{N}\\sum_{i=0}^{N-1}\\mathbf{x}_i$ is the mean of the data.\n",
    " \n",
    " The solution to this problem consists in finding the eigenvector of data covariance matrix $\\mathbf{C}$ with the largest eigenvalue. To project to $d\\geq1$ dimensions, one take the $d(\\leq D)$ eigenvectors with largest eigenvalues and aggregates them into a matrix $\\mathbf{W} = [\\mathbf{w}_{1}, \\mathbf{w}_{2}, ..., \\mathbf{w}_{d} ]$. Hence, $\\mathbf{W}$ is a matrix of $d$ eigenvectors each being $D$-dimensional. \n",
    " \n",
    "Once $\\mathbf{W}$ has been found, we can project our original data $\\mathbf{X}\\in \\mathbb{R}^{N\\times D}$ to $\\mathbf{Y}\\in \\mathbb{R}^{N\\times d}$, using the centered data $\\tilde{\\mathbf{X}}\\in R^{N\\times D}$,\n",
    "$$\\begin{align}\n",
    "        \\mathbf{Y} &= \\mathbf{\\tilde{X}}\\mathbf{W} \\\\\n",
    "        \\tilde{\\mathbf{x}}_i &= \\mathbf{x}_i-\\mathbf{\\bar{x}} ~~~~ \\text{for } 0 \\leq i \\leq N-1\n",
    "\\end{align}$$\n",
    " \n",
    "Finally, to understand how much of the variance is explained by our $d$ eigenvectors, we compute the percentage of the variance explained as \n",
    "$$\\begin{align}\n",
    "        \\mathbf{exvar} = \\frac{\\sum_{i=0}^{d-1}\\lambda_i}{\\sum_{i=0}^{D-1}\\lambda_i}\n",
    "\\end{align}$$\n",
    "where $\\lambda_i$ is the ith largest eigenvalue. For different applications, one would like to choose $d$ such that the explained variance is greater than a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are asked to code the ```PCA``` that implements the above procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, d):\n",
    "    '''\n",
    "    Input:\n",
    "        X: NxD matrix representing our data\n",
    "        d: Number of principal components to be used to reduce dimensionality\n",
    "        \n",
    "    Output:\n",
    "        mean_data: 1xD representing the mean of the input data\n",
    "        W: Dxd matrix representing the principal components\n",
    "        eg: d values representing the variance corresponding to the principal components, ie. the eigenvalues\n",
    "        Y: Nxd data projected in the principal components' direction\n",
    "        exvar: explained variance by the principal components\n",
    "    '''\n",
    "    ### WRITE YOUR CODE BELOW ###\n",
    "    # Compute the mean of data\n",
    "    mean = ...\n",
    "    # Center the data with the mean\n",
    "    X_tilde = ...\n",
    "    # Create the covariance matrix\n",
    "    C = ...\n",
    "    # Compute the eigenvectors and eigenvalues. Hint: look into np.linalg.eigh()\n",
    "    eigvals, eigvecs = ...\n",
    "    # Choose the top d eigenvalues and corresponding eigenvectors. \n",
    "    # Hint: sort the eigenvalues (with corresponding eigenvectors) in decreasing order first.\n",
    "    eigvals = ...\n",
    "    eigvecs = ...\n",
    "\n",
    "    W = ...\n",
    "    eg = ...\n",
    "\n",
    "    # project the data using W\n",
    "    Y = ...\n",
    "    \n",
    "    # Compute the explained variance\n",
    "    exvar = ...\n",
    "\n",
    "    return mean, W, eg, Y, exvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the above function and visualize the projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "mean, W, eg, Y, exvar = PCA(data, d)\n",
    "print(f'The total variance explained by the first {d} principal components is {exvar:.3f} %')\n",
    "\n",
    "assert np.isclose(exvar, 97.77, atol=0.01), 'The explained variance is not correct.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for ind,name in enumerate(cls_names):\n",
    "    filtered_class = labels==ind\n",
    "    plt.scatter(Y[filtered_class,0], Y[filtered_class,1], c=colors[ind,None], label=name)\n",
    "plt.xlabel(f'feature_0')\n",
    "plt.ylabel(f'feature_1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PCA and generally speaking:\n",
    "\n",
    "**Q.** What happens when $d=D$?\n",
    "\n",
    "**A.** There is no loss of information, but the data is projected to uncorrelated axes. (Imagine if the data was distributed in an ellipsoid, then after PCA with $d=D$, the ellipsoid would be axis-aligned so that its covariance matrix is diagonal.)\n",
    "\n",
    "**Q.** What happens when $D\\gg N$?\n",
    "\n",
    "**A.** Most of the eigenvalues will be zero as it means that most of the data reside in a subspace of dimension at maximum $N-1$. Additionally, since the computation complexity is ~$O(D^3)$, the current implementation will be slow.    \n",
    "For more explanations see Bishop on pages 569-570."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 EigenFaces\n",
    "Now, we will use PCA on images of faces. The goal is to represent faces in the dataset as a linear combination of so-called *eigenfaces*, i.e., eigenvectors of this dataset of faces that can be visualize as images.\n",
    "\n",
    "**Q.** Why can the eigenvectors be represented as images of faces?\n",
    "\n",
    "**A.** The eigenvectors are of dimension $D$, like the data. So they can be reshaped and shown as images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces().data\n",
    "print(f'Dimensions of the Face dataset: N={faces.shape[0]}, D={faces.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PCA on this dataset, and try different values of $d$ to see the impact on the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 30\n",
    "mean, W, eg, Y, exvar = PCA(faces, d)\n",
    "print(f'The total variance explained by the first {d} principal components is {exvar:.3f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Visualize\n",
    "Let us see what the mean face and the principal components look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mean.reshape(64,64), cmap='gray')\n",
    "plt.title('Mean Face');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 10 principal components\n",
    "plt.figure(figsize=(8,18))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.imshow(W.reshape(64,64,-1)[...,i], cmap='gray')\n",
    "    plt.title(f'Principal Component:{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what these components account for by adding/substracting them to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the index of the component you want to visualize, from 0 to d-1\n",
    "component_index = 0\n",
    "\n",
    "# We add the component times the weight to the mean.\n",
    "component_weights = [-5, -2, 0, 2, 5]\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "fig.suptitle(f'Principal Component {component_index}')\n",
    "for i, ax in enumerate(axs):\n",
    "    component_weight = component_weights[i]\n",
    "    # We build the image as the mean moved in the direction of a principal component\n",
    "    img = mean + W.copy()[:, component_index] * component_weight\n",
    "    ax.imshow(img.reshape(64, 64), cmap='gray')\n",
    "    ax.set_title(f'weight {component_weight}')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Can you identify what component accounts for what?\n",
    "\n",
    "**A.** The first seem to be a mix of illumination and age, the second illumination from left/right, the third changes the face width, and others, such as the 12th(0-indexed), vary from a smiling to a neutral face. \n",
    "\n",
    "While it is interesting to see what PCA finds on the principal components, we should note that these are just our interpretation as PCA has no semantic information and simply tries to look for the largest variances in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Reconstruction\n",
    "We can now project one original data sample $\\mathbf{x}_i \\in \\mathbb{R}^{D}$ to a lower-dimensional representation $\\mathbf{y}_i  \\in \\mathbb{R}^{d} $ using $\\mathbf{W} \\in \\mathbb{R}^{D\\times d}$ and $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{D}$ using the following operation: \n",
    "$$\\mathbf{y}_i = \\mathbf{W}^\\top (\\mathbf{x}_i - \\bar{\\mathbf{x}})$$\n",
    "\n",
    "From this compressed representation $\\mathbf{y}_i$, we can recover an approximation of the original data $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^{D}$ by using the opposite projection:\n",
    "$$\\hat{\\mathbf{x}}_i = \\bar{\\mathbf{x}} + \\mathbf{W}\\mathbf{y}_i$$\n",
    "\n",
    "\n",
    "Depending on how many dimension $d$ are kept, we will have some loss of information. Here we will see how changing $d$ affects the reconstruction $\\hat{\\mathbf{x}}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different values of d\n",
    "d = 10\n",
    "mean, W, eg, Y, exvar = PCA(faces, d)\n",
    "print(f'The total variance explained by the first {d} principal components is {exvar:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first select a random face from the dataset\n",
    "sample_id = np.random.choice(faces.shape[0],1)[0]\n",
    "sample_face = faces[sample_id,:]\n",
    "\n",
    "### WRITE YOUR CODE HERE: project this face to its smaller dimension representation\n",
    "proj_face = W.T @ (sample_face - mean)\n",
    "\n",
    "### WRITE YOUR CODE HERE: undo the projection (by applying W.T), \n",
    "# to recover an approximation of the initial face, from proj_face\n",
    "reconstructed_face = mean + W @ proj_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now visualize the original face, and the one reconstructed from\n",
    "# the projection on the d first eigen vectors\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.suptitle(f'Using d={d} dimensions')\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(sample_face.reshape(64,64),cmap='gray')\n",
    "ax.set_title('Original Image')\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_face.reshape(64,64),cmap='gray')\n",
    "ax.set_title('Reconstructed Image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** The formulas above are for projecting and reconstructing a data point $\\mathbf{x}_i$ using the principal component matrix $\\mathbf{W} \\in \\mathbb{R}^{D\\times d}$ and mean $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{D}$. How can you rewrite these equations to project and reconstruct an entire dataset $\\mathbf{X}\\in\\mathbb{R}^{N\\times D}$ at once? *Hint*: thinking in term of matrix dimension can help you.\n",
    "\n",
    "Assume we have $\\bar{\\mathbf{X}}\\in \\mathbb{R}^{1\\times D}$, the mean of $\\mathbf{X}$.\n",
    "\n",
    "**A.** \n",
    "First, we project the data $\\mathbf{X}$ to the lower dimensional space as\n",
    "$$\\mathbf{Y} = (\\mathbf{X} - \\bar{\\mathbf{X}}) \\mathbf{W}.$$\n",
    "\n",
    "Then, we can reconstruct the data from it with\n",
    "$$\\hat{\\mathbf{X}} = \\bar{\\mathbf{X}} + \\mathbf{Y}\\mathbf{W}^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Fisher Linear Discriminat Analysis (LDA)\n",
    "This supervised method is used to reduce dimensionality along with learning a projection, which keeps data points belonging to same class together. We will use `sklearn`'s implementation of the Fisher LDA to project MNIST data to smaller dimensions, and we will compare the results to PCA.\n",
    "\n",
    "Note: we will work with a downsampled version of MNIST ($8\\times8$ pixels) to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "mnist = datasets.load_digits()\n",
    "data = mnist.data\n",
    "labels = mnist.target\n",
    "num_class = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will project MNIST to a 2 and then a 3 dimensional space. This will allow us to visualize how the different digits are projected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 MNIST to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project with PCA\n",
    "d = 2\n",
    "mean, W, eg, Y, exvar = PCA(data, d)\n",
    "print(f'Variance explained {exvar:.3f} % (PCA)')\n",
    "proj_pca = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Project with LDA\n",
    "d = 2\n",
    "lda = LDA(n_components=d)\n",
    "# call the fit function\n",
    "obj = lda.fit(data, labels)\n",
    "# computed the variance explained using clf's parameter\n",
    "exvar = lda.explained_variance_ratio_.sum() * 100\n",
    "print(f'Variance explained {exvar:.3f} %')\n",
    "proj = obj.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(11, 5))\n",
    "fig.suptitle('Projecting MNIST to 2D')\n",
    "colors = colormaps[\"jet\"](np.linspace(0, 1, num_class))\n",
    "for i in range(num_class):\n",
    "    inds = labels == i\n",
    "    axs[0].scatter(proj_pca[inds,0], proj_pca[inds,1], color=colors[i])\n",
    "    axs[1].scatter(proj[inds,0], proj[inds,1], color=colors[i])\n",
    "axs[0].legend(np.arange(num_class))\n",
    "axs[1].legend(np.arange(num_class))\n",
    "axs[0].set_title('using PCA')\n",
    "axs[1].set_title('using Fisher LDA')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** How are the classes after the projections? Which one would you prefer if you wanted to classify the results?\n",
    "\n",
    "**A.** Because LDA uses the class labels to make the projection, it leads to a better class separation than PCA. So we would prefer to use LDA if we wanted to classify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 MNIST to 3D\n",
    "Let's repeat that but in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project with PCA\n",
    "d = 3\n",
    "mean, W, eg, Y, exvar = PCA(data, d)\n",
    "print(f'Variance explained {exvar:.3f} % (PCA)')\n",
    "proj_pca = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project with LDA\n",
    "d = 3\n",
    "lda = LDA(n_components=d)\n",
    "# call the fit function\n",
    "obj = lda.fit(data, labels)\n",
    "# computed the variance explained using clf's parameter\n",
    "exvar = lda.explained_variance_ratio_.sum() * 100\n",
    "print(f'Variance explained {exvar:.3f} %')\n",
    "proj = obj.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 5))\n",
    "fig.suptitle('Projecting MNIST to 3D')\n",
    "axs = [plt.subplot(121, projection='3d'), plt.subplot(122, projection='3d')]\n",
    "for i in range(num_class):\n",
    "    inds = labels == i\n",
    "    axs[0].scatter3D(proj_pca[inds,0], proj_pca[inds,1], proj_pca[inds,2], color=colors[i])\n",
    "    axs[1].scatter3D(proj[inds,0], proj[inds,1], proj[inds,2], color=colors[i])\n",
    "axs[0].legend(np.arange(num_class))\n",
    "axs[1].legend(np.arange(num_class))\n",
    "axs[0].set_title('using PCA')\n",
    "axs[1].set_title('using Fisher LDA')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Written questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.1** (MCQ) Which of the following statements about PCA is/are true?\n",
    "1. PCA is sensitive to initial values. A good practice is to run PCA several times with different random initializations.\n",
    "2. We should normalize the data before running PCA on it.\n",
    "3. PCA can be used to visualize high-dimensional data.\n",
    "4. PCA can be used as a linear classifier.\n",
    "\n",
    "**A.1** Correct answers are 2 and 3:\n",
    "1. *False*. There is no random initialization for PCA. Running it multiple time will give the same results.\n",
    "2. *True*. PCA looks for high variance in the data. If a feature has a very large magnitude compared to the others, then PCA will simply take it as the largest variance, which might overlook any relations between the features. Normalization before PCA is therefore good practice.\n",
    "3. *True*. We projected MNIST to 2 and 3 dimension, which can then be simply plotted.\n",
    "4. *False*. PCA does not act as a classifier and cannot make predictions. It can however be used to pre-process the data before another method for classification is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.2** Given the 2D data in the figure below, what are (approximatively) the first and second eigenvectors?\n",
    "\n",
    "<img src=\"img/pca_q2.png\" width=\"400\">\n",
    "\n",
    "**A.2** The eigenvectors are approximatively\n",
    "$$\\begin{align}\n",
    "\\mathbf{w}_1 &= \\left(\\begin{matrix} 0.90 \\\\ 0.43 \\end{matrix}\\right),\\\\\n",
    "\\mathbf{w}_2 &= \\left(\\begin{matrix} -0.43 \\\\ 0.90 \\end{matrix}\\right).\n",
    "\\end{align}$$\n",
    "\n",
    "What's important here is to understand that $\\mathbf{w}_1$ should align with the largest variance in the data, and $\\mathbf{w}_2$ is orthogonal to it and both should be of norm $1$. Additionally, the sign is not important for the eigenvectors, for example $-\\mathbf{w}_1$ could have also been the first eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.3** (True/False) If a binary classification problem is not linearly separable, we can apply PCA to project the data in a space where the problem will become linearly separable.\n",
    "\n",
    "**A.3** *False*. PCA is a *linear* dimensionalty reduction technique that can at most rotate the data (and project onto a lower dimensional space). Such transformations cannot make a non-linearly separable problem into one. Note that LDA is also a linear technique, and thus could not make the problem linearly separable, even when using the class labels!\n",
    "\n",
    "If we want to make it linearly separable, we should for example look into feature expansion (e.g., polynomial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.4** We have a dataset $\\mathbf{X} \\in \\mathbb{R}^{N\\times D}$ of $N$ samples of $D$ dimensions that we want to project to $d$ dimensions with PCA. What are the shapes of the data mean $\\bar{\\mathbf{X}}$, the covariance matrix $\\mathbf{C}$, and the projection matrix $\\mathbf{W}$?\n",
    "\n",
    "**A.4**\n",
    "* $\\bar{\\mathbf{X}}$ is of dimension $D$ (or $1\\times D$ if we think in term of numpy array).\n",
    "* $\\mathbf{C}$ is of dimension $D\\times D$.\n",
    "* $\\mathbf{W}$ is of dimension $D\\times d$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
