{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of today's session is to get you familiar with neural networks, backpropragation and training. To do that, we will not use any external libraries and we will code from scratch a multilayer perceptron (MLP).\n",
    "\n",
    "After this session, we will use standard deep learning libraries, as coding a neural network from scratch is not that easy, as you will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions introduce non-linearities in our network. They are a fundamental ingredient in a neural networks, as they allow them to learn highly non-linear mappings between the input and the target domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will look at the classical sigmoid activation function, defined as \n",
    "\n",
    "\\begin{align}\n",
    "S(z) = \\frac{1}{1 + e^{-z}}.\n",
    "\\end{align}\n",
    "\n",
    "To train our networks, we will need to compute its derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dS}{dz}(z) = \\frac{d}{dz} \\left( \\frac{1}{1 + e^{-z}} \\right) = -1 (1 + e^{-z})^{-2} (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2} = S(z)(1-S(z))\n",
    "\\end{align}\n",
    "\n",
    "Implement the sigmoid function and its gradient below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(z):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        return ...\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can test your implementation by running the code below.\n",
    "\n",
    "Pay attention to how we call the `forward` and `gradient` functions for these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10,10,1000)\n",
    "S = Sigmoid.forward(z)\n",
    "dS_dz = Sigmoid.gradient(z)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(z, S)\n",
    "plt.title('Sigmoid')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, dS_dz)\n",
    "plt.title('Derivative Sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very popular activation function, the Rectified Linear Unit (ReLU), is defined as \n",
    "\n",
    "\\begin{align}\n",
    "ReLU(z) = \\begin{cases}\n",
    "               0               & z<0\\\\\n",
    "               z               & z\\geq 0\\\\ \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "and its (sub-)derivative reads:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d ReLU}{dz}(z) = \\begin{cases}\n",
    "               0               & z<0\\\\\n",
    "               1               & z\\geq 0\\\\ \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Implement this function and its derivative below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(z):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        return ...\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can test your implementation by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10,10,1000)\n",
    "T = ReLU.forward(z)\n",
    "dT_dz = ReLU.gradient(z)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(z, T)\n",
    "plt.title('ReLU')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, dT_dz)\n",
    "plt.title('Derivative of ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define our first neural network through the `Network` class.\n",
    "\n",
    "We implemented for you the constructor of the class, which takes as input a specification of the dimension of each layer $l$ and a list of per-layer activation functions (so that each layer can have a different activation function). \n",
    "\n",
    "The initializations for our network are done in the following way:\n",
    "\n",
    "$$\\begin{align}\n",
    "b^{(l)}_j &= 0 \\, \\, \\, j=1,\\dots,J \\\\\n",
    "W^{(l)}_{ji} &= \\frac{\\mathcal N(0,1)}{\\sqrt{I}}  \\, \\, \\, i=1,\\dots,I \\, \\, \\,  j=1,\\dots,J\n",
    "\\end{align}$$\n",
    "\n",
    "where $I, J$ are the number of neurons respectively at the input/output of the $l$-th layer. $W^{(l)}_{ji}$ denotes the **weight** from node $i$ to $j$ in layer $l$ and $b^{(l)}_j$ is the **bias** term for layer $l$, node $j$.\n",
    "\n",
    "You will have to implement the forward pass of the network, which generates its output $\\mathbf{y}$.\n",
    "Specifically, the input is fed to the network and recursively multiplied with the weights of every layer $l$.\n",
    "The mathematics of the forward pass are the same for every layer $l$. \n",
    "Using the notation we introduced, this can be written as \n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{a}^{(l)} &= \\mathbf{W}^{(l)} \\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l)}\\;. \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Followed by the nonlinear activation \n",
    "$$\\begin{align}\n",
    "\\mathbf{z}^{(l)} &= \\sigma^{(l)}(\\mathbf{a}^{(l)})\n",
    "\\end{align}$$\n",
    "where $\\sigma^{(l)}$ denotes the activation function for layer $l$, which is applied independently to every element of the vector $\\mathbf{a}^{(l)}$. $\\mathbf{z}^{(l)}$ is then the output of layer $l$, and we denote by $\\mathbf{z}^{(0)}=\\mathbf{x}$ the input (i.e., we can think of the input to the network as the output of a layer $0$).\n",
    "\n",
    "Implement the forward pass in the method `feed_forward()` below, which takes as input $\\mathbf{x}$, returns $\\mathbf{a}$ and $\\mathbf{z}$, two dictionaries containing the outputs and activations for every layer. Keeping track of computed quantities will be useful for training the model. \n",
    "\n",
    "Method `predict()`, which we have implemented for you, is just a wrapper around `feed_forward()` yielding the output of our network. \n",
    "\n",
    "**NOTE:** we use batched input $\\mathbf{X}$, i.e., it is not a single vector but a matrix of multiple data points of shape $(N\\times D)$. In that case, the computation of each layer's activation can be re-written as:\n",
    "$$\\begin{align}\n",
    "\\mathbf{A}^{(l)} &= \\mathbf{Z}^{(l-1)} \\mathbf{W}^{(l)\\top} + \\mathbf{b}^{(l)}\\;. \\\\\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, dimensions, activations):\n",
    "        \"\"\"\n",
    "        :param dimensions: list of dimensions of the neural net. (input, hidden-layer, ..., hidden-layer, output)\n",
    "        :param activations: list of activation functions. Must contain N-1 activation function, where N = len(dimensions).\n",
    "        \n",
    "        Example of an MLP with 2 inputs, 10 hidden nodes and 5 outputs:\n",
    "            dimensions =  (2,       10,          5)\n",
    "            activations = (    Sigmoid,    Sigmoid)\n",
    "        which corresponds to the layers:\n",
    "            layers -->    [0,        1,          2]\n",
    "        \"\"\"\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        \n",
    "        # Weights and biases are indexed by layer l.\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "        # Activations are also indexed by layer l. \n",
    "        self.activations = {}\n",
    "\n",
    "        for l in range(1, self.n_layers):\n",
    "            self.w[l] = np.random.randn(dimensions[l], dimensions[l-1]) / np.sqrt(dimensions[l - 1])\n",
    "            self.b[l] = np.zeros(dimensions[l])\n",
    "            self.activations[l] = activations[l-1]\n",
    "\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"\n",
    "        Execute a forward feed through the network.\n",
    "        :param x: (array) Batch of input data vectors.\n",
    "        :return: (tpl) Node outputs and activations per layer. The numbering of the output is equivalent to the layer numbers.\n",
    "        \"\"\"\n",
    "        # a = Wx + b\n",
    "        a = {}\n",
    "\n",
    "        # z = sigma(a)\n",
    "        z = {0: x}  # First layer := input layer\n",
    "        \n",
    "        for l in range(1, self.n_layers):\n",
    "            # current layer = l\n",
    "            ### WRITE YOUR CODE HERE\n",
    "            a[l] = ...\n",
    "            z[l] = ...\n",
    "\n",
    "        return a, z\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :return: (array) A 2D array of shape (n_cases, n_classes).\n",
    "        \"\"\"\n",
    "        _, z = self.feed_forward(x)\n",
    "        return z[self.n_layers - 1]\n",
    "    \n",
    "\n",
    "    def back_prop(self, a, z, y_true):\n",
    "        \"\"\"\n",
    "        Back-propagation algorithm.\n",
    "        The input dicts keys represent the layers of the net.\n",
    "        :param a: (dict) Wx + b\n",
    "        :param z: (dict) sigma(a)\n",
    "        :param y_true: (array) One hot encoded ground-truth vector.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Determine partial derivative and delta for the output layer.\n",
    "        y_pred = z[self.n_layers - 1]\n",
    "        delta = self.loss_function.gradient(y_true, y_pred) * self.activations[self.n_layers - 1].gradient(y_pred)\n",
    "        \n",
    "        dw = np.dot(delta.T, z[self.n_layers - 2])\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (dw, delta)\n",
    "        }\n",
    "\n",
    "        # Determine partial derivative and delta for the rest of the layers.\n",
    "        # Each iteration requires the delta from the previous layer, propagating backwards.\n",
    "        for l in reversed(range(1, self.n_layers - 1)):\n",
    "            delta = np.dot(delta, self.w[l+1]) * self.activations[l].gradient(a[l])\n",
    "            dw = np.dot(delta.T, z[l - 1])\n",
    "            update_params[l] = (dw, delta)\n",
    "        \n",
    "        # finally update weights and biases\n",
    "        for k, v in update_params.items():\n",
    "            self.update_w_b(k, v[0], v[1])\n",
    "\n",
    "    def update_w_b(self, index, dw, delta):\n",
    "        \"\"\"\n",
    "        Update weights and biases.\n",
    "        :param index: (int) Number of the layer\n",
    "        :param dw: (array) Partial derivatives\n",
    "        :param delta: (array) Delta error.\n",
    "        \"\"\"\n",
    "        self.w[index] -= self.learning_rate * dw\n",
    "        self.b[index] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_true, loss, epochs, batch_size, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :param y_true: (array) Containing one hot encoded labels.\n",
    "        :param loss: Loss class (MSE, CrossEntropy etc.)\n",
    "        :param epochs: (int) Number of epochs.\n",
    "        :param batch_size: (int)\n",
    "        :param learning_rate: (flt)\n",
    "        \"\"\"\n",
    "        if not x.shape[0] == y_true.shape[0]:\n",
    "            raise ValueError(\"Length of x and y arrays don't match\")\n",
    "        # Initiate the loss object with the final activation function\n",
    "        self.loss_function = loss\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.arange(x.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            x_ = x[indices]\n",
    "            y_ = y_true[indices]\n",
    "\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                a, z = self.feed_forward(x_[k:l])\n",
    "                self.back_prop(a, z, y_[k:l])\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                _, z = self.feed_forward(x)\n",
    "                print(\"Loss at epoch {}: {}\".format(i + 1, self.loss_function.loss(y_true, z[self.n_layers - 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now created your first neural network, which can produce outputs based on inputs. \n",
    "To make sure everything is right, you can run the cell below. It will define a network with 1 input layer, 1 hidden layer and 1 output layer, with 1 node each. This is defined by the `dimensions` parameter, as `(1, 1, 1)`. The activation functions of the hidden layer and output layer are both sigmoid, defined by the `activations` parameter as `(Sigmoid, Sigmoid)`. \n",
    "\n",
    "We then generate a random input `x` and pass it through the `feed_forward()` function of the `Network` object. This produces two outputs `z` and `a` which are both dictionaries containing the `z` and `a` values the network computes. You will be seeing the following dictionary outputs:\n",
    "\n",
    "```python\n",
    "a:\n",
    "{\n",
    "\t1: \"values of the hidden layer\", \n",
    "\t2: \"values of the output layer\"\n",
    "}\n",
    "\n",
    "z:\n",
    "{\n",
    "\t0: \"inputs x\",\n",
    "\t1: \"activations in the hidden layer\",\n",
    "\t2: \"activations in the output layer\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize  network structure\n",
    "nn = Network(dimensions=(1, 1, 1), activations=(Sigmoid, Sigmoid))\n",
    "x = np.random.rand(1,1)\n",
    "a, z = nn.feed_forward(x)\n",
    "print(a)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions map the predictions of our network into a real number, measuring the error in our predictions.\n",
    "\n",
    "In this first session we will take the Mean Squared Error (MSE) as our loss function.\n",
    "\n",
    "Let $\\mathbf{y}_n \\in \\mathbb{R}^K$ be the prediction generated by our network for $n$-th data sample, and $\\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$ be the matrix of N predictions generated for $N$ data points.\n",
    "\n",
    "Similarly let $\\mathbf{T} \\in \\mathbb{R}^{N \\times K}$ be the matrix of true labels $\\mathbf{t}_n \\in \\mathbb{R}^ K$.\n",
    "\n",
    "We then define the Mean Squared Error (MSE) of the predictor as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{L}_{\\text{MSE}} = \\frac{1}{NC} \\sum_{n=1}^{N} \\sum_{k=1}^{C} \\left(y_{n}^{(k)}-{t_{n}^{(k)}}\\right)^{2}.\n",
    "\\end{align}$$\n",
    "\n",
    "We can compute its derivative w.r.t. the predictions $y_{n}^{(k)}$ as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{MSE}}}{\\partial y_{n}^{(k)}} = \\frac{2}{NC} \\left(y_{n}^{(k)}-{t_{n}^{(k)}}\\right).\n",
    "\\end{align}$$\n",
    "\n",
    "Re-writing this in matrix form gives\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{MSE}}}{\\partial \\mathbf{Y}} = \\frac{2}{NC} \\left(\\mathbf{Y}-\\mathbf{T}\\right).\n",
    "\\end{align}$$\n",
    "\n",
    "Implement this loss function and its derivatives in matrix form below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :param y_pred: (array) Prediction vector\n",
    "        :return: (flt)\n",
    "        \"\"\"\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        return ...\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true, y_pred):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function $\\mathcal{L}$ gives us information about the prediction error of the model. Our goal is to minimize the loss function by optimizing the weights and biases. \n",
    "\n",
    "We know from calculus that if we take the partial derivative of the loss function with respect to a certain weight $W^{(l)}_{ji}$, we get the rate of increase of $\\mathcal{L}$ in that direction. \n",
    "\n",
    "The opposite direction of $\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}_{ji}}$ will then point in the direction that minimizes the loss function.\n",
    "We can therefore, choosing a step size $\\eta$, reduce the loss function by updating our weight as follows\n",
    "$$\\begin{align}\n",
    "W^{(l)}_{ji} = W^{(l)}_{ji} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}_{ji}}\n",
    "\\end{align}$$\n",
    "This weight optimization is called gradient descent and is done w.r.t. every weight and bias of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the partial derivative of $\\mathcal{L}$ with respect to a certain weight, we can apply the chain rule in differentiation\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}\n",
    "\\end{align}$$\n",
    "\n",
    "In practice this allows us to break down the problem within each layer. We call the whole process backpropagation.\n",
    "\n",
    "\n",
    "To understand backpropagation, we will take a step back and consider a very simple neural network with one hidden layer (with a single node) taking scalar input $x \\in \\mathbb{R}$ and outputting a scalar $y \\in \\mathbb{R}$\n",
    "\n",
    "$$\\begin{align}\n",
    "a^{(1)} &= W^{(1)} x + b^{(1)} \\\\\n",
    "z^{(1)} &= \\sigma(a^{(1)}) \\\\\n",
    "a^{(2)} &= W^{(2)} z^{(1)} + b^{(2)} \\\\\n",
    "y &= \\sigma(a^{(2)}) \\\\\n",
    "\\mathcal{L} &= (y-t)^2\n",
    "\\end{align}$$\n",
    "\n",
    "Using the chain rule, compute derivatives of $\\mathcal{L}$ with respect to all weights and biases:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y} &= 2 (y-t) \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial a^{(2)}} = 2 (y-t) \\cdot \\sigma'(a^{(2)}) \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial W^{(2)}} = 2 (y-t) \\cdot \\sigma'(a^{(2)}) \\cdot z^{(1)} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial b^{(2)}} = \\cdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{(1)}} &= \\frac{\\partial \\mathcal{L}}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(1)}} = \\cdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^{(1)}} &= \\cdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} &= \\cdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} &= \\cdots\n",
    "\\end{align}$$\n",
    "\n",
    "As things get quite tricky with indices, we have implemented backpropagation for you in method `Network.back_prop()`. Look at it and convince yourself that it generalizes the derivations above to the multi-dimensional case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Fitting your model to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have implemented your first neural network, and have understood backpropagation, we will fit our model to the MNIST dataset.\n",
    "\n",
    "The MNIST database contains a total of 70,000 handwritten digits with 10 different classes, from 0 to 9. 60,000 examples are taken as training dataset and the remaining 10,000 as testing set. The digits have been size-normalized and centered in a fixed-size image. \n",
    "\n",
    "We can also sub-sample the data so that our code will run faster.\n",
    "\n",
    "First download MNIST from: https://www.python-course.eu/data/mnist/mnist_train.csv\n",
    "and https://www.python-course.eu/data/mnist/mnist_test.csv\n",
    "and put these 2 csv files in the same folder as your jupyter notebook file.\n",
    "\n",
    "To make sure everything went well, we will first load the dataset and visualize some samples from it. It might take a while to load it into memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10  # i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "train_data = np.loadtxt(\"mnist_train.csv\", delimiter=\",\")\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", delimiter=\",\") \n",
    "\n",
    "# Sub-sample the data. You can comment these lines out if you want to use the full dataset.\n",
    "train_data = train_data[::5]\n",
    "test_data = test_data[::5]\n",
    "\n",
    "x_train = np.asfarray(train_data[:, 1:])\n",
    "x_test = np.asfarray(test_data[:, 1:])\n",
    "y_train = np.asfarray(train_data[:, :1])\n",
    "y_test = np.asfarray(test_data[:, :1])\n",
    "# transform labels into one hot representation\n",
    "classes = np.arange(no_of_different_labels)\n",
    "y_train = (classes == y_train).astype(float)\n",
    "y_test = (classes == y_test).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the image and label from the training set\n",
    "for idx in range(5):\n",
    "    example_img = np.resize(x_train[idx], (28,28))\n",
    "    plt.title(f'The digit is: {y_train[idx].argmax()}')\n",
    "    plt.imshow(example_img,cmap=\"Greys\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to fit your model to MNIST. Start by considering a simple one hidden layer MLP with the following specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions =  (28*28,    20,       10)\n",
    "activations = (        ReLU,  Sigmoid)\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Network class implemented above, initialize the network and fit it to the dataset using the MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize  network structure\n",
    "nn = Network(dimensions, activations)\n",
    "# fit network to training data\n",
    "nn.fit(x_train, y_train, loss=MSE, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to previous exercise sessions, let's define a metric to measure the accuracy of our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, ground_truth):\n",
    "    match = prediction.argmax(axis=1) == ground_truth.argmax(axis=1)\n",
    "    return np.mean(match) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at how our network is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training split\n",
    "prediction = nn.predict(x_train)\n",
    "print(\"Loss on TRAIN split: {:.6f}\".format(MSE.loss(prediction, y_train)))\n",
    "print(\"Accuracy on TRAIN split: {:.1f}%\".format(accuracy(prediction, y_train)))\n",
    "\n",
    "print(\"------------------------\")\n",
    "\n",
    "# predict on testing split\n",
    "prediction = nn.predict(x_test)\n",
    "print(\"Loss on TEST split: {:.6f}\".format(MSE.loss(prediction, y_test)))\n",
    "print(\"Accuracy on TEST split: {:.1f}%\".format(accuracy(prediction, y_test)))\n",
    "\n",
    "# visualize the image and label from the training set\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    idx = np.random.randint(0, x_test.shape[0])\n",
    "    example_img = np.resize(x_test[idx],(28,28))\n",
    "    plt.title(f'Prediction={prediction[idx].argmax()} - GT={y_test[idx].argmax()}')\n",
    "    plt.imshow(example_img,cmap=\"Greys\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the architecture, activation functions and hyperparameters to increase the accuracy on the TEST split. You should be able to reach >90% accuracy! (Note that, in principle, one should *not* tune their model to the test data, but create a validation set!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Written questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.1** (MCQ) You notice that an MLP that you have been training has very low training error but very high test error. What could you do to fix this? Select all correct answers.\n",
    "\n",
    "1. We should use a validation set to determine which activation functions to use.\n",
    "2. We should try increasing the number of hidden layers.\n",
    "3. We should try increasing the number of nodes in each layer.\n",
    "4. We should increase the amount of weight decay on the weights of the network.\n",
    "\n",
    "**A.1** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.2** (MCQ) Three MLPs are shown in the figure below, with different numbers of layers and different numbers of units per layer. If the activation function is the identity function $\\sigma(x)=x$, then\n",
    "\n",
    "<img src=\"img/MLP.jpg\" width=\"400\">\n",
    "\n",
    "1. How many trainable parameters does each network have? *Note:* \"units\" is the number of neuron in a layer, i.e., it is the dimension of the layer; the first layer on the image is the input layer.\n",
    "   \n",
    "2. How well do you expect these MLP to approximate a complicated non-linear function? Order them from worst to best.\n",
    "\n",
    "**A.2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.3** Answer the following questions regarding activation functions:\n",
    "1. Can we use different activation functions for different layers?\n",
    "2. Imagine that you want to predict values in the range $[-100, 100]$. Would you use the Sigmoid activation function on the output (final) layer? What about ReLU?\n",
    "3. Can we use the Sign function as an activation function? As a reminder: $\\mathrm{sign}(x)= 1$ if $x > 0$, $0$ if $x = 0$, and $-1$ else.\n",
    "\n",
    "**A.3** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "635196d2e7d96e8482f8171ea2c3eea7d8992e2ed178bb98cf4adc6e0453abe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
