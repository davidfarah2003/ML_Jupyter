{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net is a convolutional neural network that was developed initially for biomedical image segmentation. This week, we will be working on implementing the U-Net architecture using PyTorch and performing an image segmentation task.\n",
    "\n",
    "This network is a bit more complicated than the MLP and simple CNN you have seen so far, but it is a nice example of what PyTorch allows to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be working with a small dataset of synthetic images. Let us generate 2 images with their segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images have shape (2, 1, 80, 80). Min value in image: 0. Max value in image: 255.\n",
      "Image masks have shape (2, 5, 80, 80). Min value in mask: 0.0. Max value in mask: 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(80, 80, count=2)\n",
    "\n",
    "print(f\"Input images have shape {input_images.shape}. Min value in image: {input_images.min()}. Max value in image: {input_images.max()}.\")\n",
    "print(f\"Image masks have shape {target_masks.shape}. Min value in mask: {target_masks.min()}. Max value in mask: {target_masks.max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input images have shape $(2, 1, 80, 80)$:  $2$ for the number of images, $1$ for the color channels (they are black and white in this case), and $(80,80)$ for the image height and width.\n",
    "\n",
    "The images contain 5 different shapes. For each input image we have a segmentation mask, with which we encode what shape the pixels in the image segment.\n",
    "\n",
    "The corresponding image masks have shape $(2, 5, 80, 80)$: $2$ for the number of images, $5$ for the one-hot encoding of what shape the pixel is a part of (see image below), and $(80,80)$ for the image height and width.\n",
    "\n",
    "Let us plot the image and the corresponding segmentation mask to get a better idea of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x800 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change channel-order and make 3 channels just for plotting\n",
    "input_images_rgb = [x.astype(np.uint8).repeat(3,axis=0).transpose(1,2,0) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (5ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be using PyTorch's Dataset class for preparing the data. Our `SimDataset` class will inherit from PyTorch's dataset class. We need to overwrite the functions `__len__(self)`, and `__getitem__(self, idx)`.\n",
    "\n",
    "*  `__len__(self)`: returns the size of the dataset.\n",
    "\n",
    "* `__getitem__(self, idx)`: returns the data (and its target) sample of index `idx`.\n",
    "\n",
    "\n",
    "(Hint: The tutorial at https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class is very helpful and we encourage you to have a look at it. You can also add transformations for data augmentation etc. very easily!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count):\n",
    "        # We generate our data \n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(80, 80, count=count)\n",
    "\n",
    "    def __len__(self):\n",
    "        ### WRITE YOUR CODE HERE. Return the size of the dataset\n",
    "        return self.input_images.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ### WRITE YOUR CODE HERE. Select data and mask at index idx.\n",
    "        return self.input_images[idx] , self.target_masks[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use PyTorch's `DataLoader` class. This is an iterator which allows us to batch the data, shuffle it and load it with multiprocessing workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and use PyTorch's DataLoader class.\n",
    "train_set = SimDataset(300)\n",
    "val_set = SimDataset(20)\n",
    "test_set = SimDataset(3)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=20, shuffle=True)\n",
    "val_dataloader= DataLoader(val_set, batch_size=20, shuffle=True)\n",
    "test_dataloader= DataLoader(test_set, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Dataset and Dataloader, it is time to design our deep network! Since we are doing image segmentation, we choose to use a U-Net architecture here, which was first introduced in the paper \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" ([link to paper](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) for the interested). The original architecture looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unet.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quote the architecture description from the paper directly: \n",
    "\n",
    "> It consists of a contracting path (left side) and an expansive path (right side). \n",
    "\n",
    "> The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of **two 3x3 convolutions** (unpadded convolutions), each followed by a **rectified linear unit (ReLU)** and a **2x2 max pooling operation with stride 2** for downsampling. At each downsampling step we double the number of feature channels. \n",
    "\n",
    "\n",
    "> Every step in the expansive path consists of an **upsampling** of the feature map followed by a **2x2 convolution (“up-convolution”)** that halves the number of feature channels, a **concatenation with the correspondingly cropped feature map** from the contracting path, and **two 3x3 convolutions**, each followed by a **ReLU**. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we make matters slightly simpler by using **padded convolutions with padding of size 1**, so that the image doesn't become smaller after the convolution operations. This way, **we do not have to crop** the feature map from the contracting path for the concatenation. We will also build a slightly smaller network, since our images are not so large in size to begin with. The network we want to implement is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unet-2.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets code this! We would like to first create a module class for the double convolution operation (convolution -> batch norm -> ReLU -> convolution -> batch norm -> ReLU), since it is repeated several times in the architecture. This class inherits from PyTorch's module class. We use the [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to define the layers in this module. \n",
    "\n",
    "Just to show you how this works, we wrote the class for a single convolution followed by a Sigmoid operation called `DummyConv`. We then use this `DummyConv` module three times in the `DummyNetwork`. Study how this works. \n",
    "\n",
    "**Note:** [batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) is a technique for normalizing each batch of data within the network, to help its training. If you haven't seen it in the lectures, you don't need to know its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyConv(nn.Module):\n",
    "    \"\"\"A small module that implements (convolution => sigmoid).\n",
    "    It will be re-used as a build block to make a full network.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):            \n",
    "        super().__init__()\n",
    "        \n",
    "        # We build a sequence of layers with nn.Sequential().\n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.single_conv(x)\n",
    "\n",
    "\n",
    "class DummyNetwork(nn.Module):\n",
    "    \"\"\"A network that uses the DummyConv modules above.\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.initial_layer = DummyConv(n_channels, 64)\n",
    "        self.second_layer = DummyConv(64, 128)\n",
    "        self.third_layer = DummyConv(128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.initial_layer(x)\n",
    "        x2 = self.second_layer(x1)\n",
    "        x3 = self.third_layer(x2)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to implement the `DoubleConv` module. Use the template above as help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"A module that will implement the following sequence of layers:\n",
    "    (convolution => batch normalization => ReLU => convolution => batch normalization => ReLU).\n",
    "    Batch normalization is a technique for normalization over mini-batches. \n",
    "    To use it in a CNN, you can use PyTorch's nn.BatchNorm2d layer:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "         # We build a sequence of layers with nn.Sequential().\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement the `UNet` class, as we did in previous week's exercise. We've started the implementation as a hint and left the rest for you to fill.\n",
    "\n",
    "For the **up-and-conv 2x2** operation of the network, as shown on the illustrations, we will be using **Transposed Convolutions**. You can take a look at [`nn.ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d) for the PyTorch implementation. *Hint:* You should take an interest in the arguments `kernel_size` and `stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Beginning of the \"contracting\" part of the U-Net\n",
    "        self.dc1 = DoubleConv(input_channels, 64)\n",
    "        self.mp1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.dc2 = DoubleConv(64, 128)\n",
    "        self.mp2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.dc3 = DoubleConv(128, 256)\n",
    "        self.mp3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.dc4 = DoubleConv(256, 512)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dc5 = DoubleConv(256, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dc6 = DoubleConv(128, 128)\n",
    "\n",
    "        # End of the \"expansive\" part of the U-Net\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dc7 = DoubleConv(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.dc1(x)\n",
    "        x2 = self.mp1(x1)\n",
    "        x2 = self.dc2(x2)\n",
    "\n",
    "        x3 = self.mp2(x2)\n",
    "        x3 = self.dc3(x3)\n",
    "\n",
    "        x4 = self.mp3(x3)\n",
    "        x4 = self.dc4(x4)\n",
    "\n",
    "        # End of the \"contracting\" part\n",
    "\n",
    "        # Beginning of the \"expansive\" part\n",
    "        x = self.up1(x4)\n",
    "        x = torch.cat([x, x3], dim=1)  #concatenate with x4 with the cropped part from x3 (copy and crop step)\n",
    "        x = self.dc5(x)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.dc6(x)\n",
    "                 \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.dc7(x)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(input_channels=1, n_classes=5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a sum of binary cross entropy losses and dice loss for our loss, since image segmentation is actually a classification problem (you can think of it as classifying what object the pixels belong to, if they belong to any.) You're already familiar with cross entropy loss from the lectures. Dice loss is a measure of overlap between the prediction and the ground truth labels and is also used for image segmentation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def full_loss(pred, target):\n",
    "    bce_weight = 0.5\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is given below. Fill in the missing part of the training loop! Call the `full_loss` function above to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs=10):\n",
    "    best_loss = 1e10\n",
    "    best_model_wts = None\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"LR\", param_group['lr'])\n",
    "\n",
    "        ##### TRAINING:\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0\n",
    "        epoch_samples = 0\n",
    "        #load the images and masks\n",
    "        for bi, (inputs, labels) in enumerate(train_dataloader):\n",
    "            print(f\"\\rProcessing batch {bi}/\"\n",
    "                  f\"{len(train_dataloader) - 1}\", end='')\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            ### WRITE YOUR CODE HERE: one step of the training loop.\n",
    "            outputs = model(inputs)\n",
    "            loss = full_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # statistics\n",
    "            epoch_samples +=  inputs.size(0)\n",
    "            epoch_loss += loss.data.cpu().numpy() * inputs.size(0)\n",
    "        print(\"Training epoch loss: {}\".format(epoch_loss/epoch_samples))\n",
    "        scheduler.step()\n",
    "\n",
    "        ##### VALIDATION:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "        epoch_loss = 0\n",
    "        epoch_samples = 0\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            with torch.no_grad():  # disable gradient calculation\n",
    "                outputs = model(inputs)\n",
    "                loss = full_loss(outputs, labels)\n",
    "\n",
    "                # statistics\n",
    "                epoch_loss += loss.data.cpu().numpy() * inputs.size(0)\n",
    "                epoch_samples +=  inputs.size(0)\n",
    "        print(\"Val epoch loss: {}\".format(epoch_loss/epoch_samples))\n",
    "\n",
    "        # save the model if the loss is the best\n",
    "        if epoch_loss/epoch_samples < best_loss:\n",
    "            print(\"saving best model\")\n",
    "            best_loss = epoch_loss/epoch_samples\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, it is time to train! \n",
    "\n",
    "First we will create the optimizer. This time we will use the Adam optimizer ([optim.Adam](https://pytorch.org/docs/stable/optim.html)). Set the learning rate to 1e-2.\n",
    "\n",
    "We've also created a learning rate scheduler for you, which gradually reduces the learning rate during training, this can be useful to converge to a better solution. The intuition behind it is that the closer we get to a minima, the smaller the step we want to get closer and closer.\n",
    "\n",
    "If you have a GPU with CUDA support, this should be pretty fast! However, **it might take a while if you only train on the CPU**. If this is the case, try it with a reduced number of epochs first. You should be seeing some segmented objects after 3 epochs (but keep in mind that at this point the classification will be likely wrong, i.e. visually the colors will not match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.01\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1395235 is out of bounds for axis 0 with size 300",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m optimizer_ft \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m)  \u001B[38;5;66;03m### WRITE YOUR CODE HERE\u001B[39;00m\n\u001B[0;32m      4\u001B[0m exp_lr_scheduler \u001B[38;5;241m=\u001B[39m lr_scheduler\u001B[38;5;241m.\u001B[39mStepLR(optimizer_ft, step_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexp_lr_scheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[13], line 17\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[0;32m     15\u001B[0m epoch_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m#load the images and masks\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m bi, (inputs, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dataloader):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124mProcessing batch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     19\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_dataloader) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     20\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[4], line 12\u001B[0m, in \u001B[0;36mSimDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m### WRITE YOUR CODE HERE. Select data and mask at index idx.\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_images\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m , \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_masks[idx]\n",
      "\u001B[1;31mIndexError\u001B[0m: index 1395235 is out of bounds for axis 0 with size 300"
     ]
    }
   ],
   "source": [
    "model = UNet(input_channels=1, n_classes=5)\n",
    "model = model.to(device)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-2)  ### WRITE YOUR CODE HERE\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.9)\n",
    "train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we do on the test data! We plot the ground truth masks and the predicted masks side by side below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch\n",
    "inputs, labels = next(iter(test_dataloader))\n",
    "inputs = inputs.to(device).float()\n",
    "labels = labels.to(device).float()\n",
    "\n",
    "# Predict\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.repeat(3,axis=0).transpose(1,2,0).astype(np.uint8) for x in inputs.cpu().numpy()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "print(\"We plot the input, target, and prediction for the test set:\")\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements:**\n",
    "\n",
    "This notebook contains a mix of usuyama's https://github.com/usuyama/pytorch-unet and milesial's https://github.com/milesial/Pytorch-UNet code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.1** We use a convolution with a kernel of size $K\\times K$ on an image of size $S\\times S$. \n",
    "1. What will be the size of the output if we don't use padding?\n",
    "2. So how much padding should we use to maintain the size $S\\times S$? *Hint:* how to pad each side of the image? Is there a difference when the kernel size is odd vs. even?\n",
    "\n",
    "**A.1**\n",
    "1. (S-K+1) x (S-K+1)\n",
    "2. is kernel is odd -> pad (K-1)/2 on each side . Kernel is pair one side (K-2)/2 the other side K/2 padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.2** CIFAR-10 is an image database that contains color images of size $32\\times32$, which can therefore be represented by $3\\times32\\times32$ arrays (taking the convention of putting the channels first). The goal is to classify them into 10 different classes.\n",
    "        \n",
    "To this end, we use a small Convolutional Neural Network (CNN) made of convolutional layers ($\\texttt{Conv2d}$), max-pooling layers ($\\texttt{MaxPool2d}$), and fully-connected layers ($\\texttt{FC}$), and $\\texttt{ReLU}$ activation functions. Let us parameterize these layers as follows: \n",
    "* $\\texttt{Conv2d(\\#kernels, kernel-size)}$,\n",
    "* $\\texttt{MaxPool2d(kernel-size)}$,\n",
    "* $\\texttt{FC(\\#neurons)}$. \n",
    "\n",
    "For example, a layer denoted as $\\texttt{Conv2d(8, 4)}$ consists of $8$ kernels (or filters) of size $4\\times4$. **All convolutions uses a stride of $1$ without padding.** Additionally, the feature maps can be flattened into a vector before being fed to the fully-connected layer. \n",
    "\n",
    "Given these conventions, we use the following network architecture:\n",
    "    \n",
    "$$\\texttt{Conv2d(16, 5)} \\rightarrow \\texttt{MaxPool2d(2)} \\rightarrow \\texttt{Conv2d(32, 3)} \\rightarrow \\texttt{Conv2d(32, 3)} \\rightarrow \\texttt{MaxPool2d(2)} \\rightarrow \\texttt{FC(10)}$$\n",
    "    \n",
    "What sizes are:\n",
    "1. the feature maps after the second convolution and before the third,\n",
    "2. the input to the fully-connected layer ($\\texttt{FC}$)? \n",
    "\n",
    "**A.2**\n",
    "1. 32*16 x 12 x 12\n",
    "2. 32*32 *16 x 5 x 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.3** Considering the same convention as above, but with a dataset that contains color images of size $224\\times224$, build a network to classify them into 100 classes.\n",
    "\n",
    "As constraints for building your network, you need to\n",
    "1. use the three layers presented above,\n",
    "2. have the input to the final fully-connected layers be of size $128*7*7=6'272$,\n",
    "3. and, when using $\\texttt{MaxPool2d}$, you need to make sure that the feature maps are divisible by its kernel-size!\n",
    "\n",
    "**A.3** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.4** (MCQ) Select all the correct statement(s) below regarding CNNs:\n",
    "\n",
    "1. MLPs are more robust to translations of the input than CNNs. (Imagine the image of an object, and if we translate that object over the image.)\n",
    "2. Convolutional layers cannot be used to reduce the size of feature maps by a factor of, for example, $2$.\n",
    "3. Increasing the size of the feature maps (i.e., up-sampling them) can be implemented using transposed convolutions.\n",
    "4. Amongst other things, CNNs can be used for image classification but also to output images.\n",
    "\n",
    "**A.4** 3, 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
