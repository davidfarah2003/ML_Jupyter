{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: SVM - Solutions\n",
    "\n",
    "In this exercise, we see Support Vector Machine (SVM) with various kernel functions.  \n",
    "\n",
    "We use Scikit-learn, a Python package of machine learning methods. We are using a toy binary classification example to understand Linear SVM, and then see feature expansion and kernel functions to extend it to non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plots import plot, plot_expand, plot_simple_data\n",
    "from helpers import get_circle_dataset, get_simple_dataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Scikit-Learn\n",
    "\n",
    "Training an SVM classifer is not a easy task, so in this session, we are going to use Scikit-Learn, which is a machine learning library for Python. Most of the machine learning algorithms and tools are already implemented. In this exercise, we'll use this package to train and understand SVM. If you are interested in how to optimize a SVM, you can refer to [this](https://xavierbourretsicotte.github.io/SVM_implementation.html).\n",
    "\n",
    "This package `sklearn` should already be implemented in your conda enviornment. If it's not the case, type the following command in your terminal:\n",
    "```\n",
    "conda install -y -c conda-forge sklearn\n",
    "```\n",
    "\n",
    "Scikit-Learn has modules implemented broadly for \n",
    "- Data Transformations: https://scikit-learn.org/stable/data_transforms.html\n",
    "- Model Selection and Training: https://scikit-learn.org/stable/model_selection.html\n",
    "- Supervised Techniques: https://scikit-learn.org/stable/supervised_learning.html\n",
    "- Unsupervised Techniques: https://scikit-learn.org/stable/unsupervised_learning.html\n",
    "\n",
    "All the magic happens under the hood, but gives you freedom to try out more complicated stuff.  \n",
    "Different methods to be noted here are\n",
    "- `fit`: Train a model with the data\n",
    "- `predict`: Use the model to predict on test data\n",
    "- `score`: Return mean accuracy on the given test data\n",
    "\n",
    "Have a look at [this](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting) for a simple example.\n",
    "\n",
    "We will explore SVM for classification in this session: [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). We will start here with the linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the SVM classifier class from scikit-learn.\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Linear SVM\n",
    "\n",
    "SVM tries to solve linear classification problem of this form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{w}^* = \\underset{\\mathbf{w},\\{\\xi_n\\}}{\\operatorname{argmin}}  \\ \\ & \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum^N_{n=1}\\xi_n \\\\\n",
    "    \\operatorname{subject \\  to} \\ \\ &  t_n\\cdot(\\tilde{\\mathbf{w}}\\cdot\\mathbf{x_n}) \\geq 1-\\xi_n , \\forall n \\\\\n",
    "                        &\\text{and  }\\  \\xi_n \\geq 0 , \\forall n\n",
    "\\end{align}\n",
    "$$\n",
    "where, $\\tilde{\\mathbf{w}}$ are the weights with bias term, $x_n$ is a data sample and $t_n$ is a label.\n",
    "\n",
    "**Q.** Why do we minimize $\\|\\mathbf{w}\\|$ ? \n",
    "\n",
    "**A.** $\\|\\mathbf{w}\\|$ is inversely related to margin width.\n",
    "\n",
    "**Q.** What is C? How should we choose the best value for C?\n",
    "\n",
    "**A.** C is the penalty term, $\\xi_i$ is an error in terms of how far data point is beyond the correct margin and $t_i \\in\\{-1,1\\}$ the label for binary classification. We choose the right value for C, given the data, through validation set.\n",
    "    \n",
    "**Q.** What does it mean when $\\xi_i \\gt 0$ ?\n",
    "\n",
    "**A.** Our data may not be linearly separable, hence maximizing the margin will lead to some misclassifications. $\\xi_i$ is greater than zero when a data point is within the margin, or misclassified, and how many such data points are allowed is controlled by C. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Binary Classification\n",
    "\n",
    "Let's begin with a simple **binary** classification using Linear SVM.\n",
    "The data is simply **linearly** separable.\n",
    "\n",
    "We visualize here the optimal maximum-margin solution without misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple data with 3 points per class\n",
    "X = np.array([[2,4],[1,4],[2,3],[6,-1],[7,-1],[5,-3]] )\n",
    "Y = np.array([-1, -1, -1, 1, 1, 1])\n",
    "plot_simple_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are asked to build a SVM classifier using SVC and to understand the outputs from the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# 1. Declare a SVC with C=1.0 and kernel='linear'\n",
    "clf = SVC(C=1.0, kernel='linear')\n",
    "\n",
    "# 2. use x and y to fit the model\n",
    "clf.fit(X, Y) \n",
    "\n",
    "# 3. show the fitted model\n",
    "plot(X, Y, clf)\n",
    "\n",
    "# Some information we can extract from the model\n",
    "# Take note of them as you might need them in the future!\n",
    "print('w = ', clf.coef_)\n",
    "print('w0 = ', clf.intercept_)\n",
    "print('Number of support vectors for each class = ', clf.n_support_)\n",
    "print('Support vectors = ', clf.support_vectors_)\n",
    "print('Indices of support vectors = ', clf.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we found that we have 2 **support vectors**, one in each class. They are shown highlighed in green in the plot. A support vector is a data sample that is either on the margin or within the margin (or misclassified). \n",
    "\n",
    "Let's inspect the result of the classification. We do the classification in the following way:\n",
    "\n",
    "$$ \n",
    "y_i = \\begin{cases}\n",
    "-1 & \\text{if} \\ \\mathbf{x}_i^T \\mathbf{w} + w_0 < 0\\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Note*: when doing this on multiple data points at a time, $X$ is an $N\\times D$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the weights (w) from the fitted model to predict the labels of input data points\n",
    "def raw_predict(X, w, w0):\n",
    "    '''\n",
    "    Given input data X, SVM weight w and w0, output the prediction result.\n",
    "    \n",
    "    Args:\n",
    "        X: data, array of shape (N, D) where N is the number of datapoints and D is the dimension of features.\n",
    "        w: weights, array of shape (D,)\n",
    "        w0: bias, array of shape (1,)\n",
    "    Returns:\n",
    "        out: predictions, array of shape (N,)\n",
    "    '''\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    out = np.sign(np.dot(X, w) + w0)\n",
    "    return out.astype(int)\n",
    "\n",
    "x_test = np.array([\n",
    "    [4, 2],\n",
    "    [ 6, -3]\n",
    "])\n",
    "\n",
    "### WRITE YOUR CODE HERE: Use your implementation to do the prediction on the test data.\n",
    "raw_pred = raw_predict(x_test, clf.coef_[0], clf.intercept_)\n",
    "print(\"Prediction from your implementation: \", raw_pred)\n",
    "\n",
    "### WRITE YOUR CODE HERE: Use scikit-learn's predict function to do the prediction on the test data.\n",
    "model_predict = clf.predict(x_test)\n",
    "\n",
    "print(\"Prediction from the model: \", model_predict)\n",
    "\n",
    "assert np.isclose(raw_pred, model_predict).all(), \"Your implementation is not correct.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us determine the indices of the support vectors. (Reminder: These are the data samples that fall on the margin or within the margin). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also calculate the decision function manually.\n",
    "\n",
    "## Step 1\n",
    "### WRITE YOUR CODE HERE: Code the decision function: Xw + w_0\n",
    "decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_\n",
    "\n",
    "## Step 2: We can also retrieve the decision function from the model:\n",
    "decision_function_from_model = clf.decision_function(X)\n",
    "\n",
    "assert np.isclose(decision_function, decision_function_from_model).all(), \"Your implementation is not correct.\"\n",
    "\n",
    "# What condition do the support vectors satisfy? \n",
    "# Remember that the support vectors are the points that on to the decision boundary, or within the margin.\n",
    "### WRITE YOUR CODE HERE, hint: look into np.nonzero\n",
    "support_vector_indices = np.nonzero(Y * decision_function <= 1.)\n",
    "\n",
    "print('I find the indices of support vectors = ', support_vector_indices)\n",
    "assert np.isclose(support_vector_indices, clf.support_).all(), \"Your implementation is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Different C values\n",
    "\n",
    "Let's explore the effect of $C$ on a different dataset.\n",
    "\n",
    "**Q.** How do you expect the margin to vary with C? *Hint*: have a look at the optimization formulation above.\n",
    "\n",
    "**A.** Lower C allows more misclassifications and hence lead to larger margins, while bigger C reduces misclassfications and hence lead to smaller margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the simple dataset\n",
    "X, Y = get_simple_dataset()\n",
    "plot(X, Y, None, dataOnly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, vary the C value from 0.001 to 10 and pay attention to the changes.\n",
    "\n",
    "The plot shows the decision boundary and margins of the learnt model. Encircled points are the support vectors.  \n",
    "WARNING: if the margins go beyond the limits of the axis, they might not be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a SVM model with linear kernel and PLAY WITH THE VALUE OF C\n",
    "clf = SVC(kernel='linear', C=0.01)\n",
    "\n",
    "# Call the fit method\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot(X, Y, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Reading (if interested)\n",
    "- Multiclass SVM (Bishop- Multiclass SVMs 7.1.3)\n",
    "- Can we have probabilistic interpretation of SVM? (Bishop- Relevance Support Machine 7.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Kernel SVM\n",
    "\n",
    "Beyond the linear problem we discussed before, SVM can also solve non-linear classification problem by doing some feature expansion on the input data. \n",
    "\n",
    "We replace $\\mathbf{x}_i$ with $\\phi(\\mathbf{x}_i)$, and then $\\mathbf{x}_i^\\top\\mathbf{x}_j$ with $\\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)=k(\\mathbf{x}_i,\\mathbf{x}_j)$. \n",
    "\n",
    "$\\phi(\\cdot)$ is the (possibly unknown) feature expansion function, and $k(\\cdot)$ is the kernel function.\n",
    "\n",
    "The **dual form** of this problem is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\underset{\\{\\lambda_i\\}}{\\operatorname{max}} \\ \\ \n",
    "    & \\sum_{n=1}^N \\lambda_i - \\frac 1 2 \\sum_{i=1}^N\\sum_{j=1}^N \\lambda_i\\lambda_jt_it_jk(\\mathbf{x}_i,\\mathbf{x}_j)  \\\\   \n",
    "    \\operatorname{subject \\ to} & \\ \\ \\sum_{i=1}^N \\lambda_it_i = 0 \\\\\n",
    "                 & \\ \\ 0 \\leq \\lambda_i \\leq C, \\forall i \\ \\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Q.** \n",
    "1. How can you write $\\mathbf{w}$ using $\\lambda_i$ and function $\\phi$?\n",
    "2. How is $y(\\mathbf{x})$ represented using $\\lambda_i$?\n",
    " \n",
    "**A.**\n",
    "1. $\\mathbf{w} = \\sum_{i=1}^N \\lambda_it_i\\phi(\\mathbf{x}_i) $\n",
    "2. We plugging the $\\mathbf{w}$ as, \n",
    "  $$\n",
    "  \\begin{align}\n",
    "    y(\\mathbf{x}) &= \\mathbf{w}^\\top\\phi(\\mathbf{x}) + w_0 \\\\\n",
    "                        &= \\sum_{i=1}^N \\lambda_it_ik(\\mathbf{x},\\mathbf{x_i}) + w_0\n",
    "  \\end{align}\n",
    "  $$\n",
    "   * The sum can be computed on the support vectors ($\\delta$) only:\n",
    "     $$\n",
    "     \\begin{align}\n",
    "       y(\\mathbf{x}) & = \\sum_{i \\in \\delta} \\lambda_it_ik(\\mathbf{x},\\mathbf{x_i}) + w_0\n",
    "     \\end{align}\n",
    "     $$\n",
    "\n",
    "We continue with the Scikit-Learn implementation of [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). The main parameters you should look for are:\n",
    "- Kernel Functions: `kernel`. Linear, Polynomial and RBF ($X$ is the data)\n",
    "    - Linear: `linear`. $\\langle X, X' \\rangle $.\n",
    "    - Polynomial: `poly`. $( \\gamma \\langle X, X' \\rangle + r)^d $. $d$ is specified by keyword `degree`, $r$ by `coef0`.\n",
    "    - RBF: `rbf`. $\\exp(-\\gamma ||X - X'||^2)$. $\\gamma$ is specified by keyword `gamma`, must be greater than 0.\n",
    "- Penalty term, `C`: for all\n",
    "- `gamma`: for Polynomial and RBF kernel (mostly RBF)\n",
    "- `degree`: for Polynomial kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Non-linearly separable data\n",
    "\n",
    "We use a binary dataset that cannot be separated linearly in the original feature space.\n",
    "\n",
    "Then, we'll try the different kernel and explore how their parameters affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, Y = get_circle_dataset()\n",
    "plot(X, Y, None, dataOnly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Linear SVM\n",
    "\n",
    "As you should expect, linear SVM does not perform well in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with linear kernel\n",
    "clf_linear = SVC(kernel='linear', C=1.0)\n",
    "    \n",
    "clf_linear.fit(X, Y)\n",
    "plot(X, Y, clf_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Polynomial SVM\n",
    "\n",
    "For polynomial SVM, we have two options:\n",
    "1. We can explicitely write a polynomial feature expansion $\\phi_\\text{poly}(\\cdot)$ to edit the data $X$, then use linear SVM on it.\n",
    "2. We use the kernel trick to only define a kernel function $k_\\text{poly}(\\cdot,\\cdot)$, which is directly used in SVM.\n",
    "\n",
    "Let's do both and compare the results!\n",
    "\n",
    "Fill in the function `expand_X()` that performs polynomial feature expansion. \n",
    "You should add a bias term, but **omit the interaction terms**. An example:\n",
    "\n",
    "For $D=2$ and $\\text{degree}=3$, the data\n",
    "$$\n",
    "\\mathbf{x}_i = \\begin{bmatrix}\\mathbf{x}_i^{(0)}& \\mathbf{x}_i^{(1)}\\end{bmatrix},\n",
    "$$\n",
    "after the polynomial feature expansion, will become\n",
    "$$ \n",
    "\\mathbf{\\phi}(\\mathbf{x}_i) = \\begin{bmatrix}\\mathbf{1} & \\mathbf{x}_i^{(0)} & \\mathbf{x}_i^{(1)} & (\\mathbf{x}_i^{(0)})^2 & (\\mathbf{x}_i^{(1)})^2 & (\\mathbf{x}_i^{(0)})^3 & (\\mathbf{x}_i^{(1)})^3 \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform degree-d polynomial feature expansion of input data X\n",
    "def expand_X(X, degree):\n",
    "    \"\"\"\n",
    "    Polynomial feature expansion with bias but omitting interaction terms\n",
    "    \n",
    "    Args:\n",
    "        X (array): data, shape (N, D).\n",
    "        degree (int): The degree of the polynomial feature expansion.\n",
    "    Returns:\n",
    "        expanded_X (array): Expanded data with shape (N, new_D), \n",
    "                               where new_D = D * degree + 1\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    expanded_X = np.concatenate([X**i for i in range(1, degree+1)], axis=1)\n",
    "    expanded_X = np.concatenate([np.ones((X.shape[0], 1)), expanded_X], axis=1)\n",
    "    return expanded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial SVM\n",
    "degree = 2 # you can play with the degree. How does the decision boundary change?\n",
    "\n",
    "## Do polynomial feature expansion\n",
    "### WRITE YOUR CODE HERE\n",
    "expanded_X = expand_X(X, degree)\n",
    "\n",
    "print(\"The original data has {} features.\".format(X.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(degree,expanded_X.shape[1]))\n",
    "\n",
    "## Use SVM with linear kernel on expanded data\n",
    "### WRITE YOUR CODE HERE: you can play with C\n",
    "expanded_clf = SVC(kernel='linear', C=1.0)\n",
    "expanded_clf.fit(expanded_X, Y)\n",
    "\n",
    "plot_expand(X, Y, expanded_clf, degree=degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linearly separable dataset can now be classified correctly by a linear SVM, thanks to the polynomial feature expansion.\n",
    "\n",
    "Let's now directly use the polynomial kernel function in SVM.\n",
    "\n",
    "Given data $\\mathbf{X}$ with $N$ samples, its kernel matrix $\\mathbf{K}$ is the $N \\times N$ symmetric Gram matrix with elelments \n",
    "\n",
    "$$ \\mathbf{K}_{n,m} = \\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) = k(\\mathbf{x}_n, \\mathbf{x}_m) $$\n",
    "\n",
    "The polynomial kernel is SVM is written as:\n",
    "- poly: $( \\gamma \\langle \\mathbf{X}, \\mathbf{X'} \\rangle + r)^d $. $d$ is specified by keyword `degree`, $r$ by `coef0`,\n",
    "   \n",
    "where $X$ is the data.\n",
    "\n",
    "Note that $\\phi$ **does not appear explicitly** in the kernel functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with poly kernel\n",
    "### WRITE YOUR CODE HERE and PLAY with C, degree and coef0 parameters\n",
    "clf_poly = SVC(kernel='poly', C=1.0, degree=2, coef0=1.0)\n",
    "    \n",
    "clf_poly.fit(X, Y)\n",
    "plot(X, Y, clf_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** \n",
    "1. What are the differences between polynomial feature expansion and polynomial kernel function? \n",
    "\n",
    "2. Is the SVM trained with linear kernel on polynomially expanded data same as the SVM trained with polynomial kernel function on original data?\n",
    "\n",
    "**A.** \n",
    "1. Polynomial feature expansion explicitly computes the new features $\\phi(\\mathbf{x}_i)$ and performs linear SVM in the new feature space. Polynomial kernel function uses a function $k(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)$ to compute a scalar product between two data points in the feature space, without having to explicitly send them there.\n",
    "\n",
    "2. Yes, the 2 SVMs are similar. There might be small differences based on how the polynomial kernel function is written, or how the polynomial feature expansion is done, but the results should be equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 RBF SVM\n",
    "\n",
    "Finally, let's try the Radial Basis Function (RBF) kernel:\n",
    "* `rbf`: $\\exp(-\\gamma ||X - X'||^2)$. $\\gamma$ is specified by keyword `gamma`, and must be greater than 0.\n",
    "\n",
    "$\\gamma$ is a form of scaling factor for the distance between points. If it is increased, then the exponential decays faster with distance, and vice-versa. \n",
    "\n",
    "Try different values of $\\gamma$ below, e.g., in the range $[0.01, 100]$, and see how the decision function is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with rbf kernel\n",
    "### WRITE YOUR CODE HERE and PLAY with C and gamma parameters\n",
    "clf_rbf = SVC(kernel='rbf', C=1.0, gamma=1.0)\n",
    "    \n",
    "clf_rbf.fit(X, Y)\n",
    "plot(X, Y, clf_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a kernel\n",
    "\n",
    "We have seen how to use SVM for classification as a linear classifier, and then how to extend it to non-linearly separable data through the use of kernel functions.\n",
    "\n",
    "But in practice, **how does one choose which kernel to use and its hyperparameters?**\n",
    "\n",
    "You guessed it $\\rightarrow$ by using a validation set! \n",
    "\n",
    "While in this exercise we have only tested SVM on the training data to visualize the effect of the different hyperparameters and kernels, on real problems we would also use validation data to evaluate the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Written questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Find the Margins\n",
    "\n",
    "We learn a hard-margin (i.e., *no misclassification allowed*) linear SVM classifier for the data samples shown in the figure below.\n",
    "\n",
    "<img src=\"img/svm_q1.png\" width=\"400\">\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What do you expect the decision boundary to be? What is its equation in terms of $x_1$ and $x_2$?\n",
    "\n",
    "2. How many support vectors are there? Write down their coordinates.\n",
    "\n",
    "3. What are the weights (including the bias term) for the SVM formulation? *Hint*: use the decision boundary found above and that the support vectors should lie exactly on the margin.\n",
    "\n",
    "4. If we learn a **soft-margin** (with slack variables) linear SVM classifier, then what can be the maximum and the minimum number of support vectors, assuming we play with the penatly term $C$?\n",
    "\n",
    "**Answers.**\n",
    "\n",
    "1. The decision boundary has to be perpendicular to the line joining the closest points and also divide it equally. Hence we have $x_1-x_2=0$, passing through (3,3) and the origin.\n",
    "\n",
    "2. There are two support vectors: the points (4,2) and (2,4).\n",
    "\n",
    "3. Let $w_1$, $w_2$ be the two weights and $b$ the bias. We know that (3,3) and (0,0) lie on decision boundary, so they satisfy $\\mathbf{w}^T\\mathbf{x} + w_0 = 0$. That is, \n",
    "$$\n",
    "\\begin{align} \n",
    "3w_1+3w_2+w_0 &= 0 \\\\ \n",
    "0w_1+0w_2+w_0 &= 0 \n",
    "\\end{align} \n",
    "$$\n",
    "which gives $w_1=-w_2$ and $w_0=0$. \n",
    "\n",
    "Now, since (4,2) and (2,4) are support vectors, we have\n",
    "$$ \n",
    "\\begin{align} \n",
    "4w_1+2w_2+w_0 &= 1 \\\\ \n",
    "2w_1+4w_2+w_0 &= -1 \n",
    "\\end{align}\n",
    "$$\n",
    "which gives $w_1=\\frac{1}{2}$ and $w_2=-\\frac{1}{2}$.\n",
    "\n",
    "4. We have a maximum of 4, as all data points can be support vectors when C is very small, and a minimum of 2 (one on each side of the decision boundary).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.2** (MCQ) Recall that the SVM formulation is written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\underset{\\mathbf{w},\\{\\xi_n\\}}{\\operatorname{min}}  \\ \\ & \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum^N_{n=1}\\xi_n \\\\\n",
    "    \\operatorname{subject \\  to} \\ \\ &  t_n\\cdot(\\mathbf{w}\\cdot\\mathbf{x_n} + w^{(0)}) \\geq 1-\\xi_n , \\forall n \\\\\n",
    "                        &\\text{and  }\\  \\xi_n \\geq 0 , \\forall n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $C$ is a constant, $\\xi_{n}$ are the slack variables, $\\mathbf{x}_n$ is a data sample, $t_n$ is a target label in $\\{-1,1\\}$, $w^{(0)}$ is the bias term, and $\\mathbf{w}$ are the parameters of the model (without the bias). \n",
    "\n",
    "Which of the following statements regarding this formulation is/are correct?\n",
    "1. Minimizing $\\frac{1}{2}\\|\\mathbf{w}\\|^2$ allows us to *minimize* the margin between the decision boundary and the data points.\n",
    "2. There are as many slack variables $\\xi_n$ as there are support vectors.\n",
    "3. While training the SVM, we are optimizing for the bias $w^{(0)}$, the model parameters $\\mathbf{w}$, and the slack variables $\\xi_n$.\n",
    "4. If $C$ is $0$, the points cannot be misclassified.\n",
    "5. The minimization of $C \\sum_{n=1}^N \\xi_{n}$ aims to *minimize* the number of points that are misclassified.\n",
    "\n",
    "**A.2**\n",
    "1. *Wrong*: it's to *maximize* the margin.\n",
    "2. *Wrong*: there are as many as data points. However, there are as many *non-zero* $\\xi_n$ as support vectors.\n",
    "3. *Correct*: the optimization is over all of these parameters.\n",
    "4. *Wrong*: if $C=0$, then we do not penalize at all the misclassifications or points inside the margin. Therefore, there might be a lot of misclassifications.\n",
    "5. *Correct*: the $\\xi_n$ are slack variables that allow misclassifications or points inside the margin. By aiming to minimize them, we are trying to reduce misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
